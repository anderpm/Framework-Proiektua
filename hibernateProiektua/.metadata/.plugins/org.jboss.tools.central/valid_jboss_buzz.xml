<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>How to use Quarkus with the Service Binding Operator</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/22/how-use-quarkus-service-binding-operator" /><author><name>Ioannis Kanellos</name></author><id>3392814a-3a5d-44ed-987f-7562c22d88d2</id><updated>2021-12-22T07:00:00Z</updated><published>2021-12-22T07:00:00Z</published><summary type="html">&lt;p&gt;In the seven years since &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; was released, there have been various efforts to simplify the process of consuming and binding to services from Kubernetes clusters. While discovering a service isn't much of an issue if you employ a well-known set of conventions, getting the credentials and other details required to access that service is sometimes trickier.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://svc-cat.io"&gt;Kubernetes Service Catalog&lt;/a&gt; was an attempt to simplify provisioning and binding to services, but it seems to have lost momentum. The lack of uniformity between providers, differences in how each service communicates binding information, and the fact that developers tend to favor operators for provisioning services all made the Service Catalog hard to use in practice.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-developer/service-binding-operator"&gt;Service Binding Operator&lt;/a&gt; for Kubernetes and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; is a more recent initiative. It stays out of the way of service provisioning, leaving that to operators. Instead, it focuses on how to best communicate binding information to the application. An interesting part of the specification is the &lt;a href="https://github.com/servicebinding/spec#workload-projection"&gt;workload projection&lt;/a&gt;, which defines a directory structure that will be mounted to the application container when binding occurs in order to pass all the required binding information: type, URI, and credentials&lt;/p&gt; &lt;p&gt;Other parts of the specification are related to the &lt;code&gt;ServiceBinding&lt;/code&gt; resource, which controls which services are bound to which application, and how.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;, which already supports the workload-projection part of the Service Binding specification, recently received enhancements for service binding. In this article, you'll learn how to automatically generate a &lt;code&gt;ServiceBinding&lt;/code&gt; resource, then go through the whole process from installing operators to configuring and deploying an application.&lt;/p&gt; &lt;h2&gt;A note about the example&lt;/h2&gt; &lt;p&gt;In the example, you will use &lt;a href="https://kind.sigs.k8s.io/"&gt;kind&lt;/a&gt; to install the Service Binding Operator and the &lt;a href="https://github.com/CrunchyData/postgres-operator"&gt;Postgres Operator from Crunchy Data&lt;/a&gt;. After that, you will create a PostgreSQL cluster, and finally create a simple todo application, deploying it and binding it to the provisioned cluster. Before you begin, you may want to take a look at the &lt;a href="https://redhat-developer.github.io/service-binding-operator/userguide/getting-started/quick-start.html"&gt;Service Binding Operator Quick Start Guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Set up your clusters&lt;/h2&gt; &lt;p&gt;Begin by creating a new cluster with &lt;code&gt;kind&lt;/code&gt;. (If you've already created one, or don't use &lt;code&gt;kind&lt;/code&gt; at all, you can skip this step.)&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kind create cluster&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You'll install both of the operators used in our example through the &lt;a href="https://operatorhub.io"&gt;OperatorHub&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Install the Operator Lifecycle Manager&lt;/h3&gt; &lt;p&gt;The first step is to install the &lt;a href="https://olm.operatorframework.io/"&gt;Operator Lifecycle Manager&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.19.1/install.sh | bash -s v0.19.1 &lt;/code&gt; &lt;/pre&gt; &lt;h3&gt;Install the Service Binding Operator&lt;/h3&gt; &lt;p&gt;Next, you'll install the Service Binding Operator:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create -f https://operatorhub.io/install/service-binding-operator.yaml&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Before moving to the next step, verify the installation with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get csv -n operators -w&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;When the &lt;code&gt;phase&lt;/code&gt; of the Service Binding Operator is &lt;code&gt;Succeeded&lt;/code&gt;, you can proceed.&lt;/p&gt; &lt;h3&gt;Install the Postgres Operator&lt;/h3&gt; &lt;p&gt;Use the following command to install the Postgres Operator from Crunchy Data:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create -f https://operatorhub.io/install/postgresql.yaml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you did before, you'll want to verify the installation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get csv -n operators -w&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When the &lt;code&gt;phase&lt;/code&gt; of the operator is &lt;code&gt;Succeeded&lt;/code&gt;, you can move to the next stage.&lt;/p&gt; &lt;h2&gt;Create a PostgreSQL cluster&lt;/h2&gt; &lt;p&gt;To begin this process, create a new namespace where you'll install your cluster and application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl create ns demo kubectl config set-context --current --namespace=demo&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To create the cluster, you need to apply the following custom resource:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; apiVersion: postgres-operator.crunchydata.com/v1beta1 kind: PostgresCluster metadata: name: pg-cluster namespace: demo spec: image: registry.developers.crunchydata.com/crunchydata/crunchy-postgres-ha:centos8-13.4-0 postgresVersion: 13 instances: - name: instance1 dataVolumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi backups: pgbackrest: image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbackrest:centos8-2.33-2 repos: - name: repo1 volume: volumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi - name: repo2 volume: volumeClaimSpec: accessModes: - "ReadWriteOnce" resources: requests: storage: 1Gi proxy: pgBouncer: image: registry.developers.crunchydata.com/crunchydata/crunchy-pgbouncer:centos8-1.15-2 &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;This resource has been borrowed from the &lt;em&gt;Service Binding Operator Quick Start Guide&lt;/em&gt;. Save that file as &lt;code&gt;pg-cluster.yml&lt;/code&gt; and apply it using &lt;code&gt;kubectl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl apply -f ~/pg-cluster.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now check the pods to verify the installation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get pods -n demo&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Create the Quarkus application&lt;/h2&gt; &lt;p&gt;Next, you'll create a simple Quarkus todo application that will connect to PostgreSQL via Hibernate and Panache. The todo application is a simple rest API for creating, reading, and deleting todo entries in a PostgreSQL database. It is heavily inspired by Clement Escoffier's &lt;a href="https://github.com/cescoffier/quarkus-todo-app"&gt;Quarkus todo app&lt;/a&gt; but focuses less on presentation and more on the binding aspect.&lt;/p&gt; &lt;p&gt;Generate the application using the following Maven command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn io.quarkus.platform:quarkus-maven-plugin:2.5.0.Final:create -DprojectGroupId=org.acme -DprojectArtifactId=todo-example -DclassName="org.acme.TodoResource" -Dpath="/todo" &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Next, add all of the required extensions for connecting to PostgreSQL, generate the required Kubernetes resources, and build a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image for the application using Docker:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;./mvnw quarkus:add-extension -Dextensions="resteasy-jackson,jdbc-postgresql,hibernate-orm-panache,kubernetes,kubernetes-service-binding,container-image-docker" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this point, you need to create a simple entity:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package org.acme; import javax.persistence.Column; import javax.persistence.Entity; import io.quarkus.hibernate.orm.panache.PanacheEntity; @Entity public class Todo extends PanacheEntity { @Column(length = 40, unique = true) public String title; public boolean completed; public Todo() { } public Todo(String title, Boolean completed) { this.title = title; } } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, expose it via REST:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-java"&gt;package org.acme; import javax.transaction.Transactional; import javax.ws.rs.*; import javax.ws.rs.core.Response; import javax.ws.rs.core.Response.Status; import java.util.List; @Path("/todo") public class TodoResource { @GET @Path("/") public List&lt;Todo&gt; getAll() { return Todo.listAll(); } @GET @Path("/{id}") public Todo get(@PathParam("id") Long id) { Todo entity = Todo.findById(id); if (entity == null) { throw new WebApplicationException("Todo with id of " + id + " does not exist.", Status.NOT_FOUND); } return entity; } @POST @Path("/") @Transactional public Response create(Todo item) { item.persist(); return Response.status(Status.CREATED).entity(item).build(); } @GET @Path("/{id}/complete") @Transactional public Response complete(@PathParam("id") Long id) { Todo entity = Todo.findById(id); entity.id = id; entity.completed = true; return Response.ok(entity).build(); } @DELETE @Transactional @Path("/{id}") public Response delete(@PathParam("id") Long id) { Todo entity = Todo.findById(id); if (entity == null) { throw new WebApplicationException("Todo with id of " + id + " does not exist.", Status.NOT_FOUND); } entity.delete(); return Response.noContent().build(); } } &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Bind to the target cluster&lt;/h2&gt; &lt;p&gt;In order to bind the PostgreSQL service to the application, you must either provide a &lt;code&gt;ServiceBinding&lt;/code&gt; resource or have it generated. To have the binding generated for you, you need to provide the following service coordinates:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;apiVersion&lt;/code&gt;: &lt;code&gt;postgres-operator.crunchydata.com/v1beta1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Kind: &lt;code&gt;PostgresCluster&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Name: &lt;code&gt;pg-cluster&lt;/code&gt;, prefixed with &lt;code&gt;quarkus.kubernetes-service-binding.services.&lt;id&gt;&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can see these coordinates in the following listing:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.kubernetes-service-binding.services.my-db.api-version=postgres-operator.crunchydata.com/v1beta1 quarkus.kubernetes-service-binding.services.my-db.kind=PostgresCluster quarkus.kubernetes-service-binding.services.my-db.name=pg-cluster &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the &lt;code&gt;id&lt;/code&gt; is just used to group properties together and can be any text.&lt;/p&gt; &lt;p&gt;You also need to configure the &lt;code&gt;datasource&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.datasource.db-kind=postgresql quarkus.hibernate-orm.database.generation=drop-and-create quarkus.hibernate-orm.sql-load-script=import.sql &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This sample application will not push the image to a registry, but just loads it to the cluster, so use &lt;code&gt;IfNotPresent&lt;/code&gt; as the image pull policy:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.kubernetes.image-pull-policy=IfNotPresent&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The application properties file should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;quarkus.kubernetes-service-binding.services.my-db.api-version=postgres-operator.crunchydata.com/v1beta1 quarkus.kubernetes-service-binding.services.my-db.kind=PostgresCluster quarkus.kubernetes-service-binding.services.my-db.name=pg-cluster quarkus.datasource.db-kind=postgresql quarkus.hibernate-orm.database.generation=drop-and-create quarkus.hibernate-orm.sql-load-script=import.sql quarkus.kubernetes.image-pull-policy=IfNotPresent &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Prepare for deployment&lt;/h2&gt; &lt;p&gt;Before you deploy, you need to perform a container image build, load the image to the cluster, and generate the resource.&lt;/p&gt; &lt;p&gt;First, build the container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean install -Dquarkus.container-image.build=true -DskipTests &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that this instruction assumes that you have Docker up and running.&lt;/p&gt; &lt;p&gt;Next, you'll load the Docker image to the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kind load docker-image iocanel/todo-example:1.0.0-SNAPSHOT &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;If you're using &lt;a href="https://minikube.sigs.k8s.io/docs/start/"&gt;minikube&lt;/a&gt; instead of Docker, execute the following to rebuild the image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;eval $(minikube docker-env) &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;When using tools like &lt;code&gt;kind&lt;/code&gt; or &lt;code&gt;minikube&lt;/code&gt;, it is generally a good idea to change the image pull policy to &lt;code&gt;IfNotPresent&lt;/code&gt; as you did in this example. Doing this avoids unnecessary pulls, because most of the time the image will be loaded from the local Docker daemon.&lt;/p&gt; &lt;h2&gt;Deploy the application&lt;/h2&gt; &lt;p&gt;Next, generate the deployment manifest, including the &lt;code&gt;ServiceBinding&lt;/code&gt;, and apply them on Kubernetes:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mvn clean install -Dquarkus.kubernetes.deploy=true -DskipTests&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Now, verify that everything is up and running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl get pods -n demo -w&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Verify the installation&lt;/h2&gt; &lt;p&gt;The simplest way to verify that everything works as expected is to port forward to a local HTTP port and access the &lt;code&gt;/todo&lt;/code&gt; endpoint:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kubectl port-forward service/todo-example 8080:80&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Point your browser to &lt;a href="http://localhost:8080/todo"&gt;http://localhost:8080/todo&lt;/a&gt; and enjoy!&lt;/p&gt; &lt;h2&gt;A look ahead&lt;/h2&gt; &lt;p&gt;I am very excited about recent progress on the service binding front. In the near future, we may be able to reduce the amount of configuration necessary with the use of smart conventions (such as assuming that the custom resource name is the same as the database name unless explicitly specified otherwise) and a reasonable set of defaults (such as assuming that for PostgreSQL the default operator is Crunchy Data's Postgres Operator). In such a scenario, you could bind to services with no configuration without sacrificing flexibility or customizability. I hope you are as excited as I am by this prospect!&lt;/p&gt; &lt;p&gt;See the following resources to learn more about service binding and the Service Binding Operator:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;Announcing Service Binding Operator 1.0 GA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/12/20/service-binding-operator-the-operator-in-action"&gt;Service Binding Operator: The Operator in action&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/11/03/how-use-service-binding-rabbitmq"&gt;How to use service binding with RabbitMQ&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/22/how-use-quarkus-service-binding-operator" title="How to use Quarkus with the Service Binding Operator"&gt;How to use Quarkus with the Service Binding Operator&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ioannis Kanellos</dc:creator><dc:date>2021-12-22T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 4.1.8 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-1-8" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-1-8</id><updated>2021-12-22T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.1.8 has just been released.</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">How to handle Exceptions in JAX-RS applications</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-handle-exceptions-in-jax-rs-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-exceptions-in-jax-rs-applications" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/resteasy/how-to-handle-exceptions-in-jax-rs-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-exceptions-in-jax-rs-applications</id><updated>2021-12-21T10:02:32Z</updated><content type="html">This article will teach you how to handle Exceptions properly in RESTful Web services using JAX-RS API and some advanced options which are available with RESTEasy and Quarkus runtime. Overview of REST Exceptions As for every Java classes, REST Endpoints can throw in their code both checked Exceptions (i.e. classes extending java.lang.Exception) and unchecked (i.e. ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Prevent Python dependency confusion attacks with Thoth</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth" /><author><name>Fridolin Pokorny</name></author><id>ae654f04-c49e-4f00-acd0-060b226f40bf</id><updated>2021-12-21T07:00:00Z</updated><published>2021-12-21T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; became popular as a casual scripting language but has since evolved into the corporate space, where it is used for &lt;a href="https://developers.redhat.com/topics/data-science"&gt;data science&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/ai-ml"&gt;machine learning&lt;/a&gt; applications, among others. Because Python is a high-level programming language, developers often use it to quickly prototype applications. &lt;a href="https://docs.python.org/3/extending/extending.html"&gt;Python native extensions&lt;/a&gt; make it easy to optimize any computation-intensive parts of the application using a lower-level programming language like &lt;a href="https://developers.redhat.com/topics/c"&gt;C or C++&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;For applications that need to scale, we can use &lt;a href="https://github.com/sclorg/s2i-python-container"&gt;Python Source-to-Image tooling&lt;/a&gt; (S2I) to convert a Python application into a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; image. That image can then be orchestrated and scaled using cluster orchestrators such as &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; or &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. All of these features together provide a convenient platform for solving problems using Python-based solutions that scale, are maintainable, and are easily extensible.&lt;/p&gt; &lt;p&gt;As a community-based project, the main source of open-source Python packages is the &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; (PyPI). As of this writing, PyPI hosts more than 3 million releases, and the number of releases available continues to grow exponentially. PyPI's growth is an indicator of Python's popularity worldwide.&lt;/p&gt; &lt;p&gt;However, Python's community-driven dependency resolvers were not designed for corporate environments, and that has led to dependency management issues and vulnerabilities in the Python ecosystem. This article describes some of the risks involved in resolving Python dependencies and introduces &lt;a href="https://thoth-station.ninja/"&gt;Project Thoth&lt;/a&gt;'s tools for avoiding them.&lt;/p&gt; &lt;h2&gt;Dependency management in Python&lt;/h2&gt; &lt;p&gt;The Python package installer, &lt;a href="https://pypi.org/project/pip"&gt;pip&lt;/a&gt;, is a popular tool for resolving Python application dependencies. Unfortunately, pip does not provide a way to manage lock files for application dependencies. Pip resolves dependencies to the latest possible versions at the given point in time, so the resolution is highly dependent on the time when the resolution process was triggered. Dependency problems such as &lt;em&gt;overpinning&lt;/em&gt; (requesting too wide a range of versions) frequently introduce issues to the Python application stack.&lt;/p&gt; &lt;p&gt;To address lock file management issues, the Python community developed tools such as &lt;a href="https://pypi.org/project/pip-tools/"&gt;pip-tools&lt;/a&gt;, &lt;a href="https://pipenv.pypa.io/"&gt;Pipenv&lt;/a&gt;, and &lt;a href="https://python-poetry.org/"&gt;Poetry&lt;/a&gt;. (Our &lt;a href="https://developers.redhat.com/articles/2021/05/19/micropipenv-installing-python-dependencies-containerized-applications"&gt;article introducing micropipenv&lt;/a&gt; includes an overview of these projects.)&lt;/p&gt; &lt;p&gt;The &lt;a href="https://pypi.org/"&gt;Python Package Index&lt;/a&gt; is the primary index consulted by pip. In some cases, applications need libraries from other Python package indexes. For these, pip provides the &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/#install-index-url"&gt;--index-url&lt;/a&gt; and &lt;a href="https://pip.pypa.io/en/stable/cli/pip_install/#install-extra-index-url"&gt;--extra-index-url&lt;/a&gt; options. Most of the time, there are two primary reasons you might need to install dependencies from Python package sources other than PyPI:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Installing specific builds of packages whose features cannot be expressed using wheel tags, or that do not meet &lt;a href="https://github.com/pypa/manylinux"&gt;manylinux standards&lt;/a&gt;; e.g., the &lt;a href="https://tensorflow.pypi.thoth-station.ninja/"&gt;AVX2-enabled builds of TensorFlow&lt;/a&gt; hosted on the Python package index of the Artificial Intelligence Center of Excellence (AICoE).&lt;/li&gt; &lt;li&gt;Installing packages that should not be hosted on PyPI, such as packages specific to one company or patched versions of libraries used only for testing.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Why Python is vulnerable to dependency confusion attacks&lt;/h2&gt; &lt;p&gt;The pip options &lt;code&gt;--index-url&lt;/code&gt; and &lt;code&gt;--extra-index-url&lt;/code&gt; provide a way to specify alternate Python package indexes for resolving and installing Python packages. The first option, &lt;code&gt;--index-url&lt;/code&gt;, specifies the main Python package index for resolving Python packages, and defaults to PyPI. When you need a second package index, you can include the &lt;code&gt;--extra-index-url&lt;/code&gt; option as many times as needed. The resolution logic in pip first uses the main index, then, if the required package or version is not found there, it checks the secondary indexes.&lt;/p&gt; &lt;p&gt;Thus, although you can specify the order in which indexes are consulted, the configuration is not specified for each package individually. Moreover, the index configuration is applied for transitive dependencies introduced by direct dependencies, as well.&lt;/p&gt; &lt;p&gt;To bypass this order, application developers can manage requirements with hashes that are checked during installation and resolution to differentiate releases. This solution is unintuitive and error-prone, however. Although we encourage keeping hashes in lock files for integrity checks, they should be managed automatically using the appropriate tools.&lt;/p&gt; &lt;p&gt;Now, let’s imagine a dependency named &lt;code&gt;foo&lt;/code&gt; that a company uses on a private package index. Suppose a different package with the same name is hosted on PyPI. An unexpected glitch—such as a temporary network issue when resolving the company private package index—could lead the application to import the &lt;code&gt;foo&lt;/code&gt; package from PyPI in default setups. In the worst case, the package published on PyPI might be a &lt;a href="https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610"&gt;malicious alternative&lt;/a&gt; that reveals company secrets to an attacker.&lt;/p&gt; &lt;p&gt;This issue also applies to pip-tools, Pipenv, and Poetry). Pipenv provides a way to configure a Python package index for a specific package, but it does not enforce the specified configuration. All the mentioned dependency resolution tools treat multiple Python package indexes supplied as mirrors.&lt;/p&gt; &lt;h2&gt;Using Thoth to resolve dependency confusion&lt;/h2&gt; &lt;p&gt;&lt;a href="https://thoth-station.ninja/"&gt;Thoth&lt;/a&gt; is a project sponsored by Red Hat that takes a fresh look at the complex needs of Python applications and &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;moves the resolution process to the cloud&lt;/a&gt;. Naturally, being cloud-based has its advantages and disadvantages depending on how the tool is used.&lt;/p&gt; &lt;p&gt;Because Thoth moves dependency resolution to the cloud, a central authority can resolve application requirements. This central authority can be configured with fine-grained control over which application dependencies go into desired environments. For instance, you could handle dependencies in test environments and production environments differently.&lt;/p&gt; &lt;p&gt;Thoth's resolver pre-aggregates information about Python packages from various Python package indexes. This way, the resolver can monitor Python packages published on PyPI, on the AICoE-specific TensorFlow index, on a &lt;a href="https://www.operate-first.cloud/community-handbook/pulp/usage.md"&gt;corporate Pulp Python index&lt;/a&gt;, on the &lt;a href="https://download.pytorch.org/whl/cu111/"&gt;PyTorch CUDA 11.1 index&lt;/a&gt;, and on &lt;a href="https://download.pytorch.org/whl/cpu/"&gt;builds for CPU use&lt;/a&gt;, which the PyTorch community provides for specific cases. Moreover, the cloud-based resolver &lt;a href="https://thoth-station.ninja/docs/developers/adviser/security.html"&gt;introspects the published packages with respect to security&lt;/a&gt; or &lt;a href="https://github.com/thoth-station/cve-update-job"&gt;vulnerabilities&lt;/a&gt; (see &lt;a href="https://github.com/pypa/advisory-db"&gt;PyPA’s Python Packaging Advisory Database&lt;/a&gt;) to additionally guide a &lt;a href="https://developers.redhat.com/articles/2021/09/29/secure-your-python-applications-thoth-recommendations"&gt;secure resolution process&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Please &lt;a href="https://github.com/thoth-station/support/issues/new/choose"&gt;contact the Thoth team&lt;/a&gt; if you wish to register your own Python package index to Thoth.&lt;/p&gt; &lt;h3&gt;Solver rules in Thoth&lt;/h3&gt; &lt;p&gt;A central authority can be configured to allow or block packages or specific package releases that are hosted on the Python package indexes. This feature is called &lt;em&gt;solver rules&lt;/em&gt; and is maintained by a Thoth operator.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See &lt;a href="https://thoth-station.ninja/docs/developers/adviser/deployment.html#configuring-solver-rules"&gt;Configuring solver rules&lt;/a&gt; in the Thoth documentation for more about this topic. Also check out our &lt;a href="https://www.youtube.com/watch?v=wjMNOyGupbs"&gt;YouTube video demonstrating solver rules&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;You can use solver rules to allow the Thoth operator to specify which Python packages or specific releases can be considered during the resolution process, respecting the Python package indexes registered when a request is made to the cloud-based resolver. You can also use solver rules to block the analysis of packages that are considered too old, are no longer supported, or simply don't adhere to company policies.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; &lt;a href="https://github.com/thoth-station/support/issues/new/choose"&gt;Report issues with open source Python packages&lt;/a&gt; to help us create new solver rules.&lt;/p&gt; &lt;h3&gt;Strict index configuration&lt;/h3&gt; &lt;p&gt;Another feature in Thoth is the ability to &lt;a href="https://thoth-station.ninja/docs/developers/adviser/experimental_features.html#strict-index-configuration"&gt;configure a strict Python package index configuration&lt;/a&gt;. By default, the recommendation engine considers all the packages published on the indexes it monitors and uses a &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;reinforcement learning algorithm&lt;/a&gt; to come up with a set of packages that are considered most appropriate. However, in some situations, Thoth users want to suppress this behavior and explicitly configure Python package indexes for consuming Python packages on their own.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; If you are interested in the strict index configuration, please &lt;a href="https://thoth-station.ninja/docs/developers/adviser/experimental_features.html#strict-index-configuration"&gt;browse the documentation&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=p6fjVQ0aUPE"&gt;watch our video demonstration&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Prescriptions&lt;/h3&gt; &lt;p&gt;Thoth also supports a &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;mechanism called prescriptions&lt;/a&gt; that provides additional, detailed guidelines for package resolution. Prescriptions are analogous to manifests in Kubernetes and OpenShift. A manifest lists the desired state of the cluster, and the machinery behind the cluster orchestrator tries to create and maintain the desired state. Similarly, prescriptions provide a declarative way to specify the resolution process for the particular dependencies and Python package indexes used.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; See the &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription.html"&gt;prescriptions section&lt;/a&gt; in the Thoth documentation for more about this feature. You can also browse Thoth's &lt;a href="https://github.com/thoth-station/prescriptions/"&gt;prescriptions repository&lt;/a&gt; for prescriptions available for open source Python projects. See our &lt;a href="https://developers.redhat.com/articles/2021/09/22/thoth-prescriptions-resolving-python-dependencies"&gt;article about prescriptions&lt;/a&gt; for more insight into this concept.&lt;/p&gt; &lt;p&gt;Thoth's &lt;a href="https://developers.redhat.com/articles/2021/11/17/customize-python-dependency-resolution-machine-learning"&gt;reinforcement learning algorithm&lt;/a&gt; searches for a solution that satisfies application requirements, taking prescriptions into account. This algorithm provides the power to adjust the resolution process in whatever manner users desire. Adjustments to the resolution process can be made using &lt;a href="https://thoth-station.ninja/docs/developers/adviser/prescription/should_include.html#should-include-labels"&gt;labeled requests to the resolver&lt;/a&gt; which can pick prescriptions that match specified criteria written in YAML files. An example can be consuming all the packages solely from one package index (such as a Python package index hosted using &lt;a href="https://pulpproject.org/"&gt;Pulp&lt;/a&gt;) that hosts packages that can be considered as trusted for Thoth users.&lt;/p&gt; &lt;h2&gt;About Project Thoth&lt;/h2&gt; &lt;p&gt;As part of Project Thoth, we are accumulating knowledge to help Python developers create healthy applications. If you would like to follow project updates, please &lt;a href="https://www.youtube.com/channel/UClUIDuq_hQ6vlzmqM59B2Lw"&gt;subscribe to our YouTube channel&lt;/a&gt; or follow us on the &lt;a href="https://twitter.com/thothstation"&gt;@ThothStation Twitter handle&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/21/prevent-python-dependency-confusion-attacks-thoth" title="Prevent Python dependency confusion attacks with Thoth"&gt;Prevent Python dependency confusion attacks with Thoth&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Fridolin Pokorny</dc:creator><dc:date>2021-12-21T07:00:00Z</dc:date></entry><entry><title type="html">Intelligent data as a service (iDaaS) - Architectural introduction</title><link rel="alternate" href="http://www.schabell.org/2021/12/idaas-architetural-introduction.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2021/12/idaas-architetural-introduction.html</id><updated>2021-12-21T06:00:00Z</updated><content type="html">Part 1 - Architectural introduction The last few months we have been digging deeply into the world of healthcare architectures with a focus on presenting access to ways of mapping successful implementations for specific use cases. It's an interesting challenge in that we have the mission of creating architectural content based on common customer adoption patterns.  That's very different from most of the traditional marketing activities usually associated with generating content for the sole purpose of positioning products for solutions. When you're basing the content on actual execution in solution delivery, you're cutting out the chuff.  What's that mean? It means that it's going to provide you with a way to implement a solution using open source technologies by focusing on the integrations, structures and interactions that actually have been proven to work. What's not included are any vendor promises that you'll find in normal marketing content. Those promised that when it gets down to implementation crunch time, might not fully deliver on their promises. Enter the term Portfolio Architecture.  Let's look at these architectures, how they're created and what value they provide for your solution designs. THE PROCESS The first step is to decide the use case to start with, which in my case had to be linked to a higher level theme that becomes the leading focus. This higher level theme is not quite boiling the ocean, but it's so broad that it's going to require some division into smaller parts. In this case presented here is we are looking closer at the healthcare industry and an intelligent data as a service (iDaaS) architecture. This use case we've defined as the following: iDaaS is all about transforming the way the healthcare industry interacts with data and information. It provides an example for connecting, processing and leveraging clinical, financial, administrative, and life sciences data at scale in a consistent manner.  The approach taken is to research our existing customers that have implemented solutions in this space, collect their public-facing content, research the internal implementation documentation collections from their successful engagements, and where necessary reach out to the field resources involved.  Now on to the task at hand. WHAT'S NEXT The resulting content for this project targets the following three items. * A slide deck of the architecture for use in telling the portfolio solution story. * Generic architectural diagrams providing the general details for the portfolio solution. * A write-up of the portfolio solution in a series that can be used for a customer solution brief. An overview of this series on intelligent data as a service architecture: 1. 2. Common architectural elements 3. Example iDaaS architecture 4. Example HL7 and FHIR integration architecture 5. Example iDaaS knowledge and insight architecture Catch up on any past articles you missed by following any published links above. Next in this series, we will take a look at the generic common architectural elements for the intelligent data as a service architecture.</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">How to Integrate Keycloak for Authentication with Apache APISIX</title><link rel="alternate" href="https://www.keycloak.org/2021/12/apisix" /><author><name>Xinxin Zhu &amp; Yilin Zeng</name></author><id>https://www.keycloak.org/2021/12/apisix</id><updated>2021-12-21T00:00:00Z</updated><content type="html">This article shows you how to use OpenID-Connect protocol and Keycloak for identity authentication in Apache APISIX through detailed steps. is an open source identity and access management solution for modern applications and services. Keycloak supports Single-Sign On, which enables services to interface with Keycloak through protocols such as OpenID Connect, OAuth 2.0, etc. Keycloak also supports integrations with different authentication services, such as Github, Google and Facebook. In addition, Keycloak also supports user federation, and can import users through LDAP and Kerberos. For more information about Keycloak, please refer to the . is a dynamic, real-time, high-performance API gateway, providing rich traffic management. The project offers load balancing, dynamic upstream, canary release, circuit breaking, authentication, observability, and many useful plugins. In addition, the gateway supports dynamic plugin changes along with hot update. The OpenID Connect plugin for Apache APISIX allows users to replace traditional authentication mode with centralized identity authentication mode via OpenID Connect. HOW TO USE INSTALL APACHE APISIX INSTALL DEPENDENCIES The Apache APISIX runtime environment requires dependencies on NGINX and etcd. Before installing Apache APISIX, please install dependencies according to the operating system you are using. We provide the dependencies installation instructions for CentOS7, Fedora 31 and 32, Ubuntu 16.04 and 18.04, Debian 9 and 10, and macOS. Please refer to for more details. INSTALLATION VIA RPM PACKAGE (CENTOS 7) This installation method is suitable for CentOS 7; please run the following command to install Apache APISIX. sudo yum install -y https://github.com/apache/apisix/releases/download/2.7/apisix-2.7-0.x86_64.rpm INSTALLATION VIA DOCKER Please refer to . INSTALLATION VIA HELM CHART Please refer to . INITIALIZING DEPENDENCIES Run the following command to initialize the NGINX configuration file and etcd. make init START APACHE APISIX Run the following command to start Apache APISIX. apisix start START KEYCLOAK Here we use docker to start Keycloak. docker run -p 8080:8080 -e KEYCLOAK_USER=admin -e KEYCLOAK_PASSWORD=password -e DB_VENDOR=h2 -d jboss/keycloak:9.0.2 After execution, you need to verify that Keycloak have started successfully. docker ps CONFIGURE KEYCLOAK After Keycloak is started, use your browser to access "http://127.0.0.1:8080/auth/admin/" and type the admin/password account password to log in to the administrator console. CREATE A REALM First, you need to create a realm named apisix_test_realm. In Keycloak, a realm is a workspace dedicated to managing projects, and the resources of different realms are isolated from each other. The realm in Keycloak is divided into two categories: one is the master realm, which is created when Keycloak is first started and used to manage the admin account and create other realm. the second is the other realm, which is created by the admin in the master realm and can be used to create, manage and use users and applications in this realm. The second category is the other realm, created by admin in the master realm, where users and applications can be created, managed and used. For more details, please refer to the . CREATE A CLIENT The next step is to create the OpenID Connect Client. In Keycloak, Client means a client that is allowed to initiate authentication to Keycloak. In this example scenario, Apache APISIX is equivalent to a client that is responsible for initiating authentication requests to Keycloak, so we create a Client with the name apisix. More details about the Client can be found in . CONFIGURE THE CLIENT After the Client is created, you need to configure the Apache APISIX access type for the Client. In Keycloak, there are three types of Access Type: 1. Confidential: which is used for applications that need to perform browser login, and the client will get the access token through client secret, mostly used in web systems rendered by the server. 2. Public: for applications that need to perform browser login, mostly used in front-end projects implemented using vue and react. 3. Bearer-only: for applications that don’t need to perform browser login, only allow access with bearer token, mostly used in RESTful API scenarios. For more details about Client settings, please refer to . Since we are using Apache APISIX as the Client on the server side, we can choose either "Confidential" Access Type or "Bearer-only" Access Type. For the demonstration below, we are using "Confidential" Access Type as an example. CREATE USERS Keycloak supports interfacing with other third-party user systems, such as Google and Facebook, or importing or manually creating users using LDAP . Here we will use "manually creating users" to demonstrate. Then set the user’s password in the Credentials page. CREATE ROUTES After Keycloak is configured, you need to create a route and open the Openid-Connect plugin . For details on the configuration of this plugin, please refer to the . GET CLIENT_ID AND CLIENT_SECRET In the above configuration. * client_id is the name used when creating the Client before, i.e. apisix * client_secret should be obtained from Clients-apisix-Credentials, for example: d5c42c50-3e71-4bbbe-aa9e-31083ab29da4. GET THE DISCOVERY CONFIGURATION Go to Realm Settings-General-Endpoints, select the OpenID Endpoint Configuration link and copy the address that the link points to, for example:`http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration`. CREATE A ROUTE AND ENABLE THE PLUG-IN Use the following command to access the Apache APISIX Admin interface to create a route, set the upstream to httpbin.org, and enable the plug-in OpenID Connect for authentication. Note: If you select bearer-only as the Access Type when creating a Client, you need to set bearer_only to true when configuring the route, so that access to Apache APISIX will not jump to the Keycloak login screen. curl -XPOST 127.0.0.1:9080/apisix/admin/routes -H "X-Api-Key: edd1c9f034335f136f87ad84b625c8f1" -d '{ "uri":"/*", "plugins":{ "openid-connect":{ "client_id":"apisix", "client_secret":"d5c42c50-3e71-4bbe-aa9e-31083ab29da4", "discovery":"http://127.0.0.1:8080/auth/realms/apisix_test_realm/.well-known/openid-configuration", "scope":"openid profile", "bearer_only":false, "realm":"apisix_test_realm", "introspection_endpoint_auth_method":"client_secret_post", "redirect_uri":"http://127.0.0.1:9080/" } }, "upstream":{ "type":"roundrobin", "nodes":{ "httpbin.org:80":1 } } }' ACCESS TESTING Once the above configuration is complete, we are ready to perform the relevant access tests in Apache APISIX. ACCESS APACHE APISIX Use your browser to access . Since the OpenID-Connect plugin is enabled and bearer-only is set to false, when you access this path for the first time, Apache APISIX will redirect to the login screen configured in apisix_test_realm in Keycloak and make a user login request. Enter the User peter created during the Keycloak configuration to complete user login. SUCCESSFUL ACCESS After a successful login, the browser will again redirect the link to and will successfully access the image content. The content is identical to that of the upstream . LOGOUT After the test, use your browser to access http:/127.0.0.1:9080/logout to logout your account. Note: The logout path can be specified by logout_path in the OpenID-Connect plug-in configuration, the default is logout. SUMMARY This article shows the procedure of using OpenID-Connect protocol and Keycloak for authentication in Apache APISIX. By integrating with Keycloak, Apache APISIX can be configured to authenticate and authenticate users and application services, which greatly reduces the development work involved. For more information about the implementation of authentication in Apache APISIX, see .</content><dc:creator>Xinxin Zhu &amp; Yilin Zeng</dc:creator></entry><entry><title>Prevent auto-reboot during Argo CD sync with machine configs</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/20/prevent-auto-reboot-during-argo-cd-sync-machine-configs" /><author><name>Ishita Sequeira</name></author><id>c683a603-6c02-4203-bafe-2fa97fb1e808</id><updated>2021-12-20T07:00:00Z</updated><published>2021-12-20T07:00:00Z</published><summary type="html">&lt;p&gt;Nodes in &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; can be updated automatically through OpenShift's Machine Config Operator (MCO). A machine config is a custom resource that helps a cluster manage the complete life cycle of its nodes. When a machine config resource is created or updated in a cluster, the MCO picks up the update, performs the necessary changes to the selected nodes, and restarts the nodes gracefully by cordoning, draining, and rebooting them. The MCO handles everything ranging from the kernel to the kubelet.&lt;/p&gt; &lt;p&gt;However, interactions between the MCO and the &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps workflow&lt;/a&gt; can introduce major performance issues and other undesired behavior. This article shows how to make the MCO and the &lt;a href="https://argoproj.github.io"&gt;Argo CD&lt;/a&gt; GitOps orchestration tool work well together.&lt;/p&gt; &lt;h2&gt;Machine configs and Argo CD: Performance challenges&lt;/h2&gt; &lt;p&gt;When using machine configs as part of a GitOps workflow, the following sequence can produce suboptimal performance:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Argo CD starts a &lt;a href="https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/"&gt;sync job&lt;/a&gt; after a commit to the Git repository containing application resources.&lt;/li&gt; &lt;li&gt;If Argo CD notices a new or changed machine config while the sync operation is ongoing, MCO picks up the change to the machine config and starts rebooting the nodes to apply it.&lt;/li&gt; &lt;li&gt;If any of the nodes that are rebooting contain the Argo CD application controller, the application controller terminates and the application sync is aborted.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Because the MCO reboots the nodes in sequential order, and the Argo CD workloads can be rescheduled on each reboot, it could take some time for the sync to be completed. This could also result in undefined behavior until the MCO has rebooted all nodes affected by the machine configs within the sync.&lt;/p&gt; &lt;h2&gt;Extend the application's manifest in Git&lt;/h2&gt; &lt;p&gt;The solution to the interactions in the previous section requires you to extend the application's manifest in Git by adding PreSync and PostSync hooks to Argo CD. Argo CD provides these hooks so that you can ensure that operations of your choice are performed before and after each sync (Figure 1). As the name suggests, a PreSync hook is a job that Argo CD executes right before the sync starts. Similarly, the PostSync hook executes after a sync.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;img alt="Sync Hook Workflow" data-entity-type="file" data-entity-uuid="9b3885cd-4797-49b5-9175-e43d73cedca5" height="317" src="https://developers.redhat.com/sites/default/files/inline-images/Screen%20Shot%202021-11-24%20at%2012.06.09%20AM.png" width="943" /&gt;&lt;figcaption class="rhd-c-caption"&gt;Figure 1. Sync hook workflow.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We will use &lt;a href="https://github.com/ishitasequeira/kam-blog.git"&gt;kam-blog&lt;/a&gt; as the sample application for this demo. We have generated this application following directions in the article &lt;a href="https://developers.redhat.com/articles/2021/07/21/bootstrap-gitops-red-hat-openshift-pipelines-and-kam-cli"&gt;Bootstrap GitOps with Red Hat OpenShift Pipelines and kam CLI&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Add sync hooks to Argo CD&lt;/h3&gt; &lt;p&gt;Our PreSync job pauses the Machine Config Pool (MCP) so it does not reboot the nodes in order to apply the machine config changes. We ensure this pause by setting the flag &lt;code&gt;.spec.paused&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To insert the PreSync job, create a file named &lt;a href="https://github.com/ishitasequeira/kam-blog/blob/master/config/argocd/pre-sync-job.yaml"&gt;pre-sync-job.yaml&lt;/a&gt; and add it to the same directory as the application. The content of the file is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: batch/v1 kind: Job metadata: annotations: argocd.argoproj.io/hook: PreSync argocd.argoproj.io/hook-delete-policy: HookSucceeded name: mcp-worker-pause-job namespace: openshift-gitops spec: template: spec: containers: - image: registry.redhat.io/openshift4/ose-cli:v4.4 command: - /bin/bash - -c - | echo -n "Waiting for the MCP $MCP to converge." echo $(oc patch --type=merge --patch='{"spec":{"paused":true}}' machineconfigpool/$MCP) sleep $SLEEP echo "DONE" imagePullPolicy: IfNotPresent name: mcp-worker-pause-job env: - name: SLEEP value: "10" - name: MCP value: worker restartPolicy: Never serviceAccount: sync-job-sa &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The PostSync hook resumes the MCP so that it reboots the nodes, applying the queued or incoming machine config changes. Enable this behavior by setting the flag &lt;code&gt;.spec.paused&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;. To insert the PostSync job, create a file named &lt;a href="https://github.com/ishitasequeira/kam-blog/blob/master/config/argocd/post-sync-job.yaml"&gt;post-sync-job.yaml&lt;/a&gt; and add it to the same directory as the application. The content of the file is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: batch/v1 kind: Job metadata: annotations: argocd.argoproj.io/hook: PostSync argocd.argoproj.io/hook-delete-policy: HookSucceeded name: mcp-worker-resume-job namespace: openshift-gitops spec: template: spec: containers: - image: registry.redhat.io/openshift4/ose-cli:v4.4 command: - /bin/bash - -c - | echo -n "Waiting for the MCP $MCP to converge." sleep $SLEEP echo $(oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/$MCP) echo "DONE" imagePullPolicy: Always name: mcp-worker-resume-job env: - name: SLEEP value: "5" - name: MCP value: worker dnsPolicy: ClusterFirst restartPolicy: OnFailure serviceAccount: sync-job-sa serviceAccountName: sync-job-sa terminationGracePeriodSeconds: 30&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Add permissions for Sync Hooks&lt;/h3&gt; &lt;p&gt;In order for these jobs to execute successfully, they need permissions to manipulate machine config resources in the cluster. These permissions need to be granted using a &lt;code&gt;ServiceAccount&lt;/code&gt; and appropriate &lt;code&gt;ClusterRole&lt;/code&gt; and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; properties.&lt;/p&gt; &lt;p&gt;To add the &lt;code&gt;ServiceAccount&lt;/code&gt;, &lt;code&gt;ClusterRole&lt;/code&gt;, and &lt;code&gt;ClusterRoleBinding&lt;/code&gt; properties, create a file named &lt;a href="https://github.com/ishitasequeira/kam-blog/blob/master/config/argocd/sync-job-cluster-rbac.yaml"&gt;sync-job-cluster-rbac.yaml&lt;/a&gt; and add it to the same directory as the application. The content is:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: ServiceAccount metadata: annotations: {} name: sync-job-sa namespace: openshift-gitops --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: sync-job-sa-role rules: - apiGroups: - apiextensions.k8s.io - machineconfiguration.openshift.io resources: - machineconfigpools verbs: - get - list - patch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: sync-job-sa-rolebinding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: sync-job-sa-role subjects: - kind: ServiceAccount name: sync-job-sa namespace: openshift-gitops&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can now apply the configuration to the cluster using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -k config/argocd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After you have applied the configuration, try manually syncing the application. You should see that the PreSync and PostSync jobs have paused and unpaused the MCP as shown in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/hooks.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/hooks.png?itok=4e1Mb3AJ" width="1440" height="398" alt="The OpenShift user interface shows the actions of the PreSync and PostSync hooks." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. The OpenShift user interface shows the actions of the PreSync and PostSync hooks. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;You can also see that the MCP paused by examining its details (Figure 3).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/paused.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/paused.png?itok=_Py2jKAE" width="1440" height="870" alt="Machine Config Pool details show that it is paused." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. Machine Config Pool details show that it is paused. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Once the sync job finishes, the PostSync job unpauses the MCP and resumes all the updates to the nodes in the cluster. The MCP details show this change as well (Figure 4).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/unpaused.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/unpaused.png?itok=IcMVT1Z2" width="1440" height="890" alt="Machine Config Pool details show that it is unpaused." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Machine Config Pool details show that it is unpaused. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If the sync fails for any reason, the MCP will stay paused and won't update the nodes. To resume MCP updates, you have to manually update the MCP and set the flag &lt;code&gt;.spec.paused&lt;/code&gt; to &lt;code&gt;false&lt;/code&gt;. You can set the flag using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc patch --type=merge --patch='{"spec":{"paused":false}}' machineconfigpool/worker&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Updates to machine configs can lead to uncontrolled node reboots, termination of the sync process, and unanticipated issues in the application. The workaround in this article helps to prevent nodes from rebooting while the critical Argo CD sync operations are in progress.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/20/prevent-auto-reboot-during-argo-cd-sync-machine-configs" title="Prevent auto-reboot during Argo CD sync with machine configs"&gt;Prevent auto-reboot during Argo CD sync with machine configs&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ishita Sequeira</dc:creator><dc:date>2021-12-20T07:00:00Z</dc:date></entry><entry><title type="html">How to fix Log4j CVE-2021-44228</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-handle-cve-2021-44228-in-java-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-cve-2021-44228-in-java-applications" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-handle-cve-2021-44228-in-java-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-handle-cve-2021-44228-in-java-applications</id><updated>2021-12-20T05:14:00Z</updated><content type="html">This security bulletin discusses the recent flaws in Log4j2 library which affects some recent versions of this opensource library. Let’s see how to perform checks and mitigation actions if your Java applications are using a flawed Log4j2 version. At the time of writing (20 December 2021), Log4j has the following vulnerabilities: CVE-2021-44228 (CVSS:10) CVE-2021-45046 (CVSS:10) ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Quarkus 2.5.4.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-5-4-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-5-4-final-released/</id><updated>2021-12-20T00:00:00Z</updated><published>2021-12-20T00:00:00Z</published><summary type="html">We just released Quarkus 2.5.4.Final, a maintenance release for our 2.5 release train containing bugfixes and documentation improvements. It is a safe upgrade for anyone already using 2.5. If you are not using 2.5 already, please refer to the 2.5 migration guide. Main changes Jackson 2.12.6 We upgraded to Jackson...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2021-12-20T00:00:00Z</dc:date></entry><entry><title>How to connect Prometheus to OpenShift Streams for Apache Kafka</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/12/17/how-connect-prometheus-openshift-streams-apache-kafka" /><author><name>Pete Muir</name></author><id>8568af74-782f-4e57-8d7b-519de49b8526</id><updated>2021-12-17T07:00:00Z</updated><published>2021-12-17T07:00:00Z</published><summary type="html">&lt;p&gt;You've always been able to view some metrics for &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; in the UI or access them via the API. We recently added a feature to OpenShift Streams for Apache Kafka that exports these metrics to &lt;a href="https://prometheus.io"&gt;Prometheus&lt;/a&gt;, a system monitoring and alerting toolkit. Connecting these services lets you view your exported metrics alongside your other Kafka cluster metrics.&lt;/p&gt; &lt;p&gt;This article shows you how to set up OpenShift Streams for Apache Kafka to export metrics to Prometheus. The example configuration includes integration with &lt;a href="https://grafana.com"&gt;Grafana&lt;/a&gt;, a data visualization application that is often used with Prometheus. Almost all observability products and services support Prometheus, so you can easily adapt what you learn in this article to your observability stack.&lt;/p&gt; &lt;h2&gt;Guides and prerequisites&lt;/h2&gt; &lt;p&gt;We will use a number of guides for this configuration; You can open them now or use the links in each section:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;OpenShift Dedicated: Creating a cluster&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;Configuring a GitHub identity provider&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;OpenShift Dedicated: Managing administration roles and users&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;Getting started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We will also use these examples from the Prometheus and Grafana projects, respectively:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/example/additional-scrape-configs"&gt;Additional scrape configs for Prometheus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/GrafanaWithIngressHost.yaml"&gt;GrafanaWithIngressHost.yaml&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Set up your Kubernetes cluster&lt;/h2&gt; &lt;p&gt;To begin, you need to set up a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster to run Prometheus and Grafana. The example in this article will use &lt;a href="https://cloud.redhat.com/products/dedicated/?intcmp=701f20000012jmYAAQ"&gt;Red Hat OpenShift Dedicated&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Start by following the &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/creating_a_cluster/index"&gt;OpenShift Dedicated guide&lt;/a&gt; to creating a Customer Cloud Subscription cluster on Amazon Web Services. Then, follow the instruction to &lt;a href="https://docs.openshift.com/dedicated/identity_providers/config-identity-providers.html#config-github-idp_config-identity-providers"&gt;configure a GitHub identity provider&lt;/a&gt; so that you can use your GitHub ID to log in to &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. (Other options are available, but we're using these methods for the example.) Once you've got things configured, your OpenShift Cluster Manager should look like the screenshot in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/prometheus-fig1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/prometheus-fig1.png?itok=B6P_G0hA" width="600" height="147" alt="The OpenShift Cluster Manager with GitHub configured as an identity provider." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. Configure GitHub as an identity provider for logging in to the OpenShift Dedicated console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Finally, give your GitHub user the &lt;code&gt;cluster-admin&lt;/code&gt; role by following the OpenShift Dedicated guide to &lt;a href="https://access.redhat.com/documentation/en-us/openshift_dedicated/4/html/administering_your_cluster/osd-admin-roles"&gt;managing administration roles and users&lt;/a&gt;. Note that the Prometheus examples assume you have admin permissions on the cluster. You'll need to use your GitHub username as the principal here.&lt;/p&gt; &lt;p&gt;Now, you can log in to the console using the &lt;strong&gt;Open Console&lt;/strong&gt; button.&lt;/p&gt; &lt;h2&gt;Install Prometheus and Grafana&lt;/h2&gt; &lt;p&gt;You can install Prometheus on OpenShift Dedicated via the OperatorHub. Just navigate to &lt;strong&gt;Operators -&gt; OperatorHub&lt;/strong&gt;, filter for Prometheus, click &lt;strong&gt;Install&lt;/strong&gt;, and accept the defaults. You can validate that it's installed in the &lt;strong&gt;Installed Operators&lt;/strong&gt; list. Once that's done, do the same for Grafana.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: You might have command-line muscle memory and prefer to use &lt;code&gt;kubectl&lt;/code&gt; with a Kubernetes cluster. If you want to take this route, switch to a terminal and copy the login command from the OpenShift console user menu to set up your Kubernetes context.&lt;/p&gt; &lt;h2&gt;Set up Prometheus&lt;/h2&gt; &lt;p&gt;To get Prometheus working with OpenShift Streams for Apache Kafka, use the examples in the Prometheus documentation to create an &lt;a href="https://github.com/prometheus-operator/prometheus-operator/tree/main/example/additional-scrape-configs"&gt;additional scrape config&lt;/a&gt;. You will need to make a couple of modifications to your configuration.&lt;/p&gt; &lt;h3&gt;Create an additional config for Prometheus&lt;/h3&gt; &lt;p&gt;First, create the additional config file for Prometheus. To do this, create a file called &lt;code&gt;prometheus-additional.yaml&lt;/code&gt; with the following content:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- job_name: "kafka-federate" static_configs: - targets: ["api.openshift.com"] scheme: "https" metrics_path: "/api/kafkas_mgmt/v1/kafkas/&lt;Your Kafka ID&gt;/metrics/federate" oauth2: client_id: "&lt;Your Service Account Client ID&gt;" client_secret: "Your Service Account Client Secret" token_url: "https://identity.api.openshift.com/auth/realms/rhoas/protocol/openid-connect/token&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The angle brackets (&lt;code&gt;&lt;&gt;&lt;/code&gt;) in the listing indicate details you'll need to gather from your own OpenShift Streams for Apache Kafka environment:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;You can see your Kafka ID by clicking on your &lt;strong&gt;Kafka Instance&lt;/strong&gt; in the OpenShift Streams for Apache Kafka console.&lt;/li&gt; &lt;li&gt;Follow OpenShift Streams for Apache Kafka &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_streams_for_apache_kafka/1/guide/f351c4bd-9840-42ef-bcf2-b0c9be4ee30a"&gt;getting started guide&lt;/a&gt; to create a Kafka instance and service account for each instance. As described in the guide, copy and paste the client ID and client secret into &lt;code&gt;prometheus-additional.yaml&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Create a Kubernetes secret&lt;/h3&gt; &lt;p&gt;Now, you need to create a Kubernetes secret that contains this config file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;kubectl create secret generic additional-scrape-configs --from-file=prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f - -n &lt;namespace&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Apply your changes&lt;/h3&gt; &lt;p&gt;Finally, apply &lt;code&gt;prometheus.yaml&lt;/code&gt;, &lt;code&gt;prometheus-cluster-role-binding.yaml&lt;/code&gt;, &lt;code&gt;prometheus-cluster-role.yaml&lt;/code&gt;, and &lt;code&gt;prometheus-service-account.yaml&lt;/code&gt; using &lt;code&gt;kubectl apply -f &lt;filename&gt;&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Set up Grafana&lt;/h2&gt; &lt;p&gt;To get Grafana working, use the &lt;a href="https://github.com/grafana-operator/grafana-operator/blob/master/deploy/examples/GrafanaWithIngressHost.yaml"&gt;GrafanaWithIngressHost.yaml&lt;/a&gt; example code from the Grafana project's GitHub repository. Remove the &lt;code&gt;hostname&lt;/code&gt; field from the file, as OpenShift Dedicated will assign the hostname automatically.&lt;/p&gt; &lt;p&gt;Now, find the URL for Grafana from the OpenShift console &lt;strong&gt;Routes&lt;/strong&gt; section, and open Grafana. The login details for Grafana are in the Grafana custom resource.&lt;/p&gt; &lt;p&gt;Next, connect Grafana to Prometheus by navigating to &lt;strong&gt;Settings -&gt; Data Sources&lt;/strong&gt;. Click &lt;strong&gt;Add data source&lt;/strong&gt;, then click &lt;strong&gt;Prometheus&lt;/strong&gt;. At that point, all you need to do is enter &lt;code&gt;http://prometheus-operated:9090&lt;/code&gt;, the URL of the service, then click &lt;strong&gt;Save &amp; Test&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;You should now find metrics for your Kafka cluster in Grafana. Figure 2 shows a Grafana dashboard that displays most of the metrics available with OpenShift Streams for Apache Kafka.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/prometheus-fig2.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/prometheus-fig2.jpg?itok=dF75OCFB" width="600" height="516" alt="A Grafana dashboard for Red Hat OpenShift Streams for Apache Kafka." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. A sample Grafana dashboard for OpenShift Streams for Apache Kafka. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The JSON that defines this dashboard is &lt;a href="https://github.com/pmuir/rhosak-grafana-dashboard"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, I've shown you how to use Prometheus and Grafana to observe an OpenShift Streams for Apache Kafka instance. Prometheus is a very widely adopted format for monitoring and can be used with almost all observability services and products.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/12/17/how-connect-prometheus-openshift-streams-apache-kafka" title="How to connect Prometheus to OpenShift Streams for Apache Kafka"&gt;How to connect Prometheus to OpenShift Streams for Apache Kafka&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Pete Muir</dc:creator><dc:date>2021-12-17T07:00:00Z</dc:date></entry></feed>
